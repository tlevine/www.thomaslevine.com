---
title: News Hack Day SF
created_at: 2012-06-02
kind: article
---

Blogging about [News Hack Day SF](http://newshackdaysf.tumblr.com)
[so](http://www.civicplayground.com/2012/06/25/we-liberated-the-data-at-newshack/)
[three](http://allthingsd.com/20120626/it-may-not-be-televised-but-the-journalism-revolution-will-be-hacked/)
[weeks](http://newshackdaysf.tumblr.com/post/25857744845/thank-you-newshack-day-wraps-up)
[ago](http://kiranb.scripts.mit.edu/blog/?p=359),
but indulge me nonetheless as I brag about what I did four weekends ago.

## Before the weekend

I arrived in San Francisco on Tuesday and stayed with a friend in his
crazy warehouse in Oakland until Thursday, attending a nerdy talk every night.

On Thursday Brittney and I prepared some things for the Friday at the Chronicle.
Then we had lunch at a hilarious diner. Look at their authentic retro wifi!

![]()

It's so retro that the waitresses take orders on iPads!
It's like they had to go over-the-top with the dinerness in order that the
internet access and iPads would be acceptable.
Oh and the food was quite awesome.

I stayed the rest of the nights at the StartupHouse. That was rather crazy as well
(in a different sense than the converted warehouse sense).

## Friday
We had our scraping tutorial on Friday morning. The turnout was pretty good.

![Picture](aoeu)

If you are jealous about your non-attendance, you can take a look at the
[video](foo) and the [slides](http://scraperwiki.thomaslevine.com).

Sam Lippman and Tim West made us some rather tasty lunch after the tutorial,
then we broke into eight groups, nominally to work on particular projects,
but really to fiddle around with computers and hang out.

[Michael](http://www.majorplanetstudios.org), [Michael](http://www.mikejcorey.com),
John and [I](http://thomaslevine.com) hovered around helping various groups.
I helped a bunch of journalists with more elementary programming things,
but I also helped some people who had particular projects in mind.

One person had come across some Microsoft Access databases of San Francisco health inspections
posted online. We looked into ways of converting them to more convenient
table formats and pondered things to do with them. If you are bored, I
recommend that you post all of the health inspection reports on Foursquare or Yelp.

One group was looking at a particular table in tax filings, so I
had some fun pulling that table out with them.

Another group huddled in the corner and then announced at the end that they
had managed to extract quotes from press releases with bearable reliability.
They continued working on this over the rest of the weekend.

## Weekend

Through the weekend, Michael, Michael, John and I continued hovering around
helping various groups.

I wound up mostly helping people with some elementary programming and web
development stuff. Maybe that'll get them started on learning more about
that and making cool things.

People always ask me how to save tweets, so I also showed off the function
that I discussed in an earlier post.

Tim West made more awesome food all weekend long. Look at our Sunday breakfast.

[![https://p.twimg.com/AwKriuTCIAEB6Me.jpg](fawaffle)](http://t.co/FAiD8oIL)

I even got to tackle one of the hardest problems in Computer Science. As I was
working with one group, it became clear that the group wanted to make a clone
of Needlebase. We had figured out the easier parts--what we want in general,
what to do first, how to do it, &c. But what would we name it?

>  "There are only two hard problems in Computer Science: cache invalidation and naming things."
>  Phil Karlton

Considering Needlebase's name, my wisdom led me "Haystack". And Randall
hipstamatized this by turning it into "Haystacks" and replacing "stacks"
with "stax". So "Haystax".

## Data Projects

Since ScraperWiki does stuff with "data", let's talk about the weekend's "data" projects.

That group that had huddled in the corner on Friday huddled in a different
corner over the weekend. On Sunday, they unveiled their creation,
[On the Record](https://github.com/alexredstone/on-the-record).
It shows lets you search quotes from news articles by person over time.

[Haystax](http://haystaxdata.org) created enough functionality to scrape one
of the test case sites identified by Reporters Lab. And for this, they won
the ScraperWiki prize.

And if there had been a 404 page prize [Bird Dog](http://birddogit.herokuapp.com)
would have [won](http://birddogit.herokuapp.com/404.html).

## More

After all this craziness, I left San Francisco the following Monday night
for ScraperWiki's Liverpool office, which is where I am right now. Quite
hilariously, this is the first time I've ever been to the office or anywhere
in England.
